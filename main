import requests
import re
from time import sleep
import json
from bs4 import BeautifulSoup
import colorama
from Test import html_obj
good = colorama.Fore.GREEN + '[*]' + colorama.Fore.RESET
bad = colorama.Fore.RED + '[!]' + colorama.Fore.RESET


HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',
        'Accept-Encoding': 'none',
        'Accept-Language': 'en-US,en;q=0.8',
        'Connection': 'keep-alive'}



def domain_enumeration(domain):

    try:
        print(good, 'Requesting domain...')
        sleep(.5)

        #requests_call = requests.get('https://' + domain, headers=HEADERS)
        #html_object = requests_call.content.__str__()
        html_object = html_obj.__str__()
        print(good, 'Domain extraction...')
        sleep(.5)

        data = re.findall(r'(http|ftp|https):\/\/([\w\-_]+(?:(?:\.[\w\-_]+)+))([\w\-\.,@?^=%&:/~\+#]*[\w\-\@?^=%&/~\+#])?', html_object)

        malicious_string = ['xxx', 'porn', 'sex', 'xnxx', 'xvideo']
        print(good, 'Domain enumeration...')
        sleep(.5)

        print('Domain List:')
        sleep(1)
        positive_domains = []
        non_positive_domains = []

        for url_data in data:
            trigger = None

            protocol = url_data[0]
            domain = url_data[1]
            route = url_data[2]

            for string in malicious_string:
                if string in domain + route:
                    positive_domains.append(domain + route)
                    trigger = True
                    if domain[:4] == 'www.':
                        print(bad, domain[4:] + route)
                        sleep(.3)
                    else:
                        print(bad, domain + route)
                        sleep(.3)
                else:
                    pass
            if trigger == None:
                non_positive_domains.append(domain + route)
                if domain[:4] == 'www.':
                    print(good, domain[4:] + route)
                    sleep(.3)
                else:
                    print(good, domain + route)
                    sleep(.3)

        if len(positive_domains) > 0:
            print("""
            Palpe0 found {} positive domains
            Palpe0 found {} non positive domains
            """.format(len(positive_domains), len(non_positive_domains)))



        # print(protocol, domain, route)
        #  if domain not in domains_data:
        #   if route == '/':
        #        domains_data[domain]={'protocol':protocol, 'route':[]}
        #     else:
        #          domains_data[domain] = {'protocol': protocol, 'route': [route,]}
        #   else:
        # if route == '/':
        #      continue
        #   else:
        #        if route in domains_data[domain]['route']:
        #             continue
        #          else:
        #               domains_data[domain]['route'].append(route)
        print(good, 'Saving on database')
        #with open(domain,'a') as file:
        #  file.write(str(domains_data) + '\n')
        #   file.close()
        #print(good, 'done!')
    except (requests.exceptions.ConnectionError, requests.exceptions.MissingSchema) as err:
        e = err.__str__()
        if e[:18] == 'HTTPConnectionPool' or e[:19] == 'HTTPSConnectionPool':
            print(bad, 'Failed to establish a new connection, domain not available!')
        elif e[2:20] == 'Connection aborted':
            print(bad, 'An existing connection has been forced to break by the remote host')
        else:
            print(bad, e)
    except KeyboardInterrupt:
        print()
        print('Palpe0 Stopped...')
